#+title: Homework 3
#+date: Tuesday, February 24, 2022
#+options: toc:nil
#+bibliography: main.bib
#+latex_header: \usepackage{enumitem}
#+latex_header: \setlist[enumerate,1]{label=$\alph*)$}
#+latex_header: \usepackage{amsthm}
#+latex_header: \usepackage{tikz}
#+latex_header: \usetikzlibrary{arrows,intersections}
#+latex_header: \allowdisplaybreaks
#+latex_header: \everymath{\displaystyle}

#+begin_src latex-macros
\newenvironment{problem}{\begin{itshape}}{\end{itshape}}
\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\randsamp}{X_1, \ldots, X_n}
\newcommand{\iid}{\randsamp \sim\text{iid}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\text{Var}}
#+end_src

* Exercise 1

#+begin_problem
Let $X_1, X_2, \ldots, X_n$ be a random sample from a Gaussian distribution with mean
$\mu=0$ and variance $\sigma^2 < \infty$. Show that $\sum_{i=1}^n X_i^2$ is a sufficient
statistic for $\sigma^2$.
#+end_problem

#+begin_solution
Let's take our usual distribution $f(x | \theta)$ and $\theta = \sigma^2$, (keeping in mind $\mu
= 0$)

\begin{eqnarray*}
f(x | \theta) & =& \prod_{i=1}^n (2\pi\sigma^2)^{\frac{-1}{2}} \exp \left( - \frac{(x_i - \mu)^2}{2\sigma^2} \right) \\
& =& (2\pi\sigma^2)^{\frac{-n}{2}} \exp \left( - \sum_{i=1}^n \frac{(x_i)^2}{2\sigma^2} \right) \\
& =& (2\pi\sigma^2)^{\frac{-n}{2}} \exp \left( \frac{-1}{2\sigma^2} \sum_{i=1}^n x_i^2 \right) \\
& =& (2\pi\sigma^2)^{\frac{-n}{2}} \exp \left( \frac{-1}{2\sigma^2} T(x) \right) \\
\end{eqnarray*}

So, we can see that this is now a sufficient statistic.
#+end_solution

* Exercise 6.5.7

#+begin_problem
Let $f(x, y | \theta_1, \theta_2, \theta_3, \theta_4)$ be the bivariate pdf for the uniform
distribution on the rectangle with lower left corner $(\theta_1, \theta_2)$ and the upper
right corner $(\theta_3, \theta_4)$ in $\mathbb{R}^2$. The parameters satisfy $\theta_1 < \theta_3$ and $\theta_2
< \theta_4$. Let $(X_1, Y_1), \ldots, (X_n, Y_n)$ be a random sample from this pdf. Find a
four-dimensional sufficient statistic for $(\theta_1, \theta_2, \theta_3, \theta_4)$.
#+end_problem

#+begin_solution
Let's take a look at,

\begin{tikzpicture}
  \coordinate (O) at (0,0);

  \draw[->] (-0.3,0) -- (8,0) coordinate (xmax);
  \draw[->] (0,-0.3) -- (0,5) coordinate[label = {right:$\mathbb{R}^2$}] (ymax);
  \path[name path=x] (0.3,0.5) -- (6.7,4.7);
  \path[name path=y] plot[smooth] coordinates {(-0.3,2) (2,1.5) (4,2.8) (6,5)};

  \scope[name intersections = {of = x and y, name = i}]
    \fill[gray!20] (i-1) -- (i-2 |- i-1) -- (i-2) -- (i-1 |- i-2);
    \draw (i-1) node[label = {south west:$(\theta_1, \theta_2)$}] (i-1) {};
    \path (i-2) node[label = {north east:$(\theta_3, \theta_4)$}] (i-2) {}
    -- (i-2 |- i-1) node (i-12) {};
    \draw[blue, <->] (i-2) -- node[right] {$|\theta_4 - \theta_2|$} (i-12);
    \draw[blue, <->] (i-1) -- node[below] {$|\theta_3 - \theta_1|$} (i-12);

    \node (area) at (8,4.4) {$A=(\theta_3 - \theta_1)(\theta_4 - \theta_2)$};
    \draw[->] (area.west) to[bend right] (3,2.5);
  \endscope
\end{tikzpicture}

#+end_solution

* Exercise 6.5.12

#+begin_problem
A natural ancillary statistic in most problems is the /sample size/. For
example, let $N$ be a random variable taking values $1, 2, \ldots$ with known
probabilities $p_1, p_2, \ldots$, where $\sum p_i = 1$. Having observed $N = n$, perform
$n$ Bernoulli trials with success probability $\theta$, getting $X$ successes.

1) Prove that the pair $(X, N)$ is minimal sufficient and $N$ is ancillary for
   $\theta$. (Note the similarity to some of the hierarchical models discussed in
   Section 4.4).
1) Prove that the estimator $X/N$ is unbiased for $\theta$ and has variance $\theta(1-\theta)\E(1/N)$.
#+end_problem

* Exercise 6.5.15

#+begin_problem
Let $\iid \, n(\theta, a\theta^2)$, where $a$ is a known constant and $\theta > 0$.

1) Show the parameter space does not contain a two-dimensional open set.
1) Show that the statistic $T = (\Xbar, S^2)$ is a sufficient statistic for $\theta$,
   but the family of distributions is not complete.
#+end_problem

#+begin_solution
$\\$
1) The parameter space of $n(\theta, a\theta^2)$ has only one independent variable and can
   we represented by the graph of $y = a\theta^2 \,\, \theta > 0$ (just a demonstration for $a>0$)

\begin{tikzpicture}
  \draw[->] (-0.2,0) -- (4,0) node[right] {$\theta$};
  \draw[->] (0,-0.2) -- (0,4) node[above] {$y$};

  \draw[red, thick] (0,0) parabola bend (0,0) (4,4) node[below right] {$a\theta^2$};
\end{tikzpicture}

This graph is an one-dimensional object embedded in two-dimensional space
therefore it has no two-dimensional open subsets.

1) [@2] Let's write the pdf for the normal distribution,

\begin{equation*}
f(X_i=x_i|\theta) = \frac{1}{\sqrt{2\pi a \theta^2}} \exp{\left( - \frac{(x_i-\theta)^2}{2a\theta^2} \right)}
\end{equation*}

And for the joint distribution,

\begin{eqnarray*}
f(X_1=x_1, \ldots, \X_n=x_n|\theta) & =& \prod_{i=1}^n \frac{1}{\sqrt{2\pi a\theta^2}} \exp{\left( -\frac{(x_i-\theta)^2}{2a\theta^2} \right)} \\
& =& \left(2\pi a\theta^2\right)^{\frac{-n}{2}} \exp{ \left( - \frac{\sum_{i=1}^n (x_i-\theta)^2}{2a\theta^2} \right)} \\
\end{eqnarray*}

Let us consider just the term $\sum_{i=1}^n (x_i-\theta)^2$ by using the familiar adding
and subtracting of $\xbar$,

\begin{eqnarray*}
\sum_{i=1}^n (x_i-\theta)^2 & = & \sum_{i=1}^n (x_i- \xbar + \xbar - \theta)^2\\
& =& \sum_{i=1}^n (x_i- \xbar)^2 + 2(x_i - \xbar)(\xbar - \theta) + (\xbar - \theta)^2\\
& =& \sum_{i=1}^n (x_i- \xbar)^2 + 2(\xbar - \theta) \sum_{i=1}^n (x_i - \xbar) + \sum_{i=1}^n (\xbar - \theta)^2\\
\end{eqnarray*}

Recall that $\sum_{i=1}^n (x_i - \xbar) = 0$, so we now have,

\begin{eqnarray*}
\sum_{i=1}^n (x_i-\theta)^2 & =& \sum_{i=1}^n (x_i- \xbar)^2 + \sum_{i=1}^n (\xbar - \theta)^2\\
& =& \sum_{i=1}^n (x_i- \xbar)^2 + n (\xbar - \theta)^2\\

\end{eqnarray*}

Also, recall that the sample variance is,
$S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \Xbar)^2$,

\begin{eqnarray*}
\sum_{i=1}^n (x_i-\theta)^2 & =&  (n-1)s^2 + n (\xbar - \theta)^2 \\
\end{eqnarray*}

Using this result in our joint distribution, we now have

\begin{eqnarray*}
f(X_1=x_1, \ldots, \X_n=x_n|\theta) & =& \left(2\pi a\theta^2\right)^{\frac{-n}{2}} \exp{ \left( - \frac{\sum_{i=1}^n (x_i-\theta)^2}{2a\theta^2} \right)} \\
& =& \left(2\pi a\theta^2\right)^{\frac{-n}{2}} \exp{ \frac{-1}{2a\theta^2} \left( (n-1)s^2 + n (\xbar - \theta)^2 \right) }
\end{eqnarray*}

Since this is of the form $g(T_1(x), T_2(x) | \theta) \cdot h(x)$, where

\begin{eqnarray*}
T_1(x) & =& \Xbar \\
T_2(x) & =& S^2 \\
h(x) & =& 1
\end{eqnarray*}

By the Factorization Theorem (p. 279 [cite:@casella2021statistical]) $T(x) =
(\Xbar, S^2)$ is a
sufficient statistic.

Now, we show that that $T$ is not complete. We will do this by constructing a
$g(T)$ where $\E_{\theta}(g(T)) = 0 \,\, \forall \, \theta$ such that $g(T) = 0$ isn't implied.
We know that $\E(S^2) = a\theta^2$. Recall the alternative form of variance,

\begin{equation}
\label{eq:altvar}
\var{(\Xbar)} = \E(\Xbar^2) - \left( \E(\Xbar) \right)^2
\end{equation}

But we know that,

\begin{eqnarray*}
\var(\Xbar) & =& \var{\left( \frac{1}{n} (X_1 + \dots + X_n) \right)} \\
& =& \frac{1}{n^2} \left( \var{(X_1)} + \dots + \var{(X_n)}) \\
& =& \frac{1}{n^2} \left( a\theta^2 + \dots + a\theta^2 \right) \\
& =& \frac{a\theta^2}{n}
\end{eqnarray*}

Rewriting $\eqref{eq:altvar}$ we have,

\begin{eqnarray*}
\E(X^2) & =& \var(\Xbar) + \left(\E(\Xbar)\right)^2 \\
& =& \frac{a\theta^2}{n} + \theta^2 \\
& =& \frac{a + n}{n} \theta^2
\end{eqnarray*}

This guides us to choose,

\begin{equation}
\label{eq:g}
g(\Xbar, S^2) = \frac{n}{a + n} \Xbar^2 - \frac{S^2}{a}
\end{equation}

Now, we calculate,

\begin{eqnarray*}
\E(g(\Xbar, S^2)) & =& \E\left( \frac{n}{a + n} \Xbar^2 - \frac{S^2}{a} \right) \\
& =& \frac{n}{a + n} \E(\Xbar^2) - \frac{1}{a} \E(S^2) \\
& =& \frac{n}{a + n} \left( \frac{a + n}{n} \theta^2 \right) - \frac{1}{a} \left( a\theta^2 \right) \\
& =& \theta^2 - \theta^2 \\
& =& 0
\end{eqnarray*}

So, we have $\E(g) = 0$ for all $\theta$ but, of course, $P(g(T) = 0)$ is not
necessarily $1$.
#+end_solution

* Exercise 6.5.19

#+begin_problem
The random variable $X$ takes the values $0, 1, 2$ according to one of the
following distributions:

#+caption: Distributions
#+name: tab:dists
|                | $P(X = 0)$ | $P(X = 1)$ | $P(X = 2)$    |                       |
|----------------+------------+------------+---------------+-----------------------|
| Distribution 1 | $p$        | $3p$       | $1 - 4p$      | $0 < p < \frac{1}{4}$ |
| Distribution 2 | $p$        | $p^2$      | $1 - p - p^2$ | $0 < p < \frac{1}{2}$ |

In each case determine whether the family of distributions of $X$ is complete.
#+end_problem

* Exercise 6.5.21

#+begin_problem
Let $X$ be one observation from the pdf

\begin{equation}
\label{eq:foo}
f(X|\theta) = \left( \frac{\theta}{2} \right)^{|x|} (1 - \theta)^{1 - |x|}, \quad x=-1, 0, 1, \quad 0 \le \theta \le 1.
\end{equation}

1) Is $X$ a complete sufficient statistic?
1) Is $\left| X \right|$ a complete sufficient statistic?
1) Does $f(x | \theta)$ belong to the exponential class?
#+end_problem

* Exercise 6.5.30

#+begin_problem
Let $\randsamp$ be a random sample from the pdf $f(x|\mu) = e^{-(x-\mu)}$, where $-\infty
< \mu < x < \infty$.

1) Show that $X_{(1)} = \min_i X_i$ is a complete sufficient statistic.
1) Use Basu's Theorem to show that $X_{(1)}$ and $S^2$ are independent.
#+end_problem

* Exercise 6.5.36

#+begin_problem
One advantage of using a minimal sufficient statistic is that unbiased
estimators will have smaller variance, as the following exercise will show.
Suppose that $T_1$ is sufficient and $T_2$ is minimal sufficient, $U$ is an
unbiased estimator of $\theta$, and define $U_1 = \E(U | T_1)$ and $U_2 = \E(U | T_2)$.

1) Show that $U_2 = \E(U_1  | T_2)$.
1) Now use the conditional variance formula (Theorem 4.4.7
[cite:@casella2021statistical]) to show that $\var \, U_2 \le \var \, U_1$.
#+end_problem

#+print_bibliography:
