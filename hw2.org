#+title: Homework 2
#+date: Tuesday, February 11, 2022
#+options: toc:nil
#+latex_header: \usepackage{enumitem}
#+latex_header: \setlist[enumerate,1]{label=$\alph*)$}
#+latex_header: \usepackage{amsthm}
#+latex_header: \newenvironment{problem}{\begin{itshape}}{\end{itshape}}
#+latex_header: \newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}
#+latex_header: \newcommand{\Xbar}{\overline{X}}
#+latex_header: \newcommand{\prob}{\text{Pr}}
#+latex_header: \newcommand{\randsamp}{X_1, \ldots, X_n}
#+latex_header: \newcommand{\iid}{\randsamp \sim\text{iid}}
#+latex_header: \newcommand{\ve}{\varepsilon}
#+latex_header: \allowdisplaybreaks
#+latex_header: \everymath{\displaystyle}

* Exercise 5.7.13

#+begin_problem
Let $\iid\, N(\mu, \sigma^{2})$ . Find a function of $S^{2}$, the same variance, say
$g(S^{2})$, that satisfies $Eg(S^{2}) = \sigma$. (/Hint/: Try $g(S^{2}) = c
\sqrt{S^{2}}$ where $c$ is constant.)
#+end_problem

#+begin_solution
From the hint, we have $g(S^2) = c \sqrt{S^2}$. Since this is a sample from a
normal distribution,

\begin{eqnarray*}
E\left( g(S^2) \right) &=& E\left( c \sqrt{S^2} \right)
                        = c \sqrt{\frac{\sigma^2}{n - 1}} E\left( \sqrt{\frac{S^2(n-1)}{\sigma}} \right) \\
&=& c \sqrt{\frac{\sigma^2}{n - 1}} \int_0^{\infty} \sqrt{t} \frac{1}{\Gamma(\frac{n-1}{2})2^{(n-1)/2}} t^{\left( \frac{n-1}{2} \right) - 1} e^{-t/2} dt.
\end{eqnarray*}

Recognize that this is almost the $\chi^2$ distribution, which we can obtain by
multiplying by something equivalent to $1$,

\begin{eqnarray*}
E\left( c \sqrt{S^2} \right) &=& c \sqrt{\frac{\sigma^2}{n - 1}} \cdot \frac{\Gamma(n/2)2^{n/2}}{\Gamma(\frac{n-1}{2})2^{(n-1)/2}}
\int_0^{\infty} \frac{1}{\Gamma(n/2)2^{n/2}} t^{\left( \frac{n-1}{2} \right) - 1/2} e^{-t/2} dt \\
&=&  c \sqrt{\frac{\sigma^2}{n - 1}} \cdot \frac{\Gamma(n/2)2^{1/2}}{\Gamma(\frac{n-1}{2})}
\int_0^{\infty} \frac{1}{\Gamma(n/2)2^{n/2}} t^{\left( \frac{n-1}{2} \right) - 1/2} e^{-t/2} dt \\
&=&  c \sqrt{\frac{\sigma^2}{n - 1}} \cdot \frac{\Gamma(n/2)2^{1/2}}{\Gamma(\frac{n-1}{2})}
\end{eqnarray*}

Therefore, to have this equal $\sigma$, we need $c$ to be,

\(
c = \sqrt{\frac{n-1}{2}} \cdot \frac{\Gamma(\frac{n-1}{2})}{\Gamma(n/2)}.
\)

#+end_solution
* Exercise 5.7.16

#+begin_problem
Let $X_{i}, i = 1, 2, 3$, be independent with $N(i, i^{2})$ distributions. For
each of the following situations, use $X_{i}$'s to construct a statistic with
the indicated distribution,

1) chi squared with 3 degrees of freedom
1) t distribution with 2 degrees of freedom
1) F distribution with 1 and 2 degrees of freedom
#+end_problem

#+begin_solution
$\\$
1) Given our $X_i$ as above, let $Z_i = \frac{X_i - i}{\sqrt{i^2}}$, which is a
   standard normal variable. Now, if we sum the square of $Z_i$ it will be a
   $\chi^{2}$ distribution with $3$ degrees of freedom,
   \begin{equation*}
  \sum_{i=1}^{3} \left( \frac{X_i - i}{\sqrt{i^2}} \right)^2 \sim \chi_3^2
 \end{equation*}

 1) [@2] Fischer's $t$ is given by $t = \frac{Z}{\sqrt{\chi^2/n}}$, so we have

\begin{equation*}
\frac{
    \left( \frac{X_i-1}{i} \right)
}{
    \sqrt{\sum_{i=2}^3 \left( \frac{X_i-i}{i} \right)^2/2}
}
\end{equation*}

1) [@3] Just square the part from (b)
#+end_solution

* Exercise 5.7.21

#+begin_problem
What is the probability that the larger of two continuous $iid$ random variables
will exceed the population median? Generalize this result to samples of size $n$.
#+end_problem

#+begin_solution
Note: taking "exceed" to mean "greater than," i.e. "$>$"

Let $m$ be our median and $X_{1}, X_{2}$ be our two continuous =iid= random
variables. Recall that the median is defined (p. 78) as $\prob(X \le m) =
\frac{1}{2}$ and $\prob(X \ge m) = \frac{1}{2}$.

A useful insight into this problem is that "larger of two continuous =iid=
random variables," e.g. $\max(X_{1}, X_{2})$, means "both $X_{1}, X_{2}$ are
greater than $m$, $X_{1}$ is greater than $m$ but $X_{2}$ is not, or $X_{2}$ is
greater than $m$ but $X_{1}$ is not. " Just as in exercise $5.7.1$, it is much
easier to take the complement of this, which in this case means "both $X_{1}$ and
$X_{2}$ are less than or equal to $m$."

In other words, our insight can be written as,
\begin{eqnarray*}
\prob(\max(X_{1}, X_{2}) > m) & = & 1 - \prob(X_{1},X_{2} \le m).
\end{eqnarray*}

Now, we just use the fact that $X_{1}, X_{2}$ are =iid=,

\begin{eqnarray*}
\prob(X_{1},X_{2} \le m) & = & \prob(X_{1} \le m) \cdot \prob(X_{2} \le m).
\end{eqnarray*}

By definition, $\prob(X_{i} \le m) = \frac{1}{2}$. Therefore,

\begin{eqnarray*}
\prob(\max(X_{1}, X_{2}) > m) & = & 1 - \prob(X_{1},X_{2} \le m) \\
& = & 1 - \prob(X_{1} \le m) \cdot \prob(X_{2} \le m) \\
& = & 1 - \left( \frac{1}{2} \cdot \frac{1}{2} \right) \\
& = & \frac{3}{4}.
\end{eqnarray*}

Now, let's generalize this for samples of size $n$. Let $\randsamp$ be our
random sample of size $n$. Using the same reasoning as before,
\begin{eqnarray*}
\prob(\max(\randsamp) > m) & = & 1 - \prob(\randsamp \le m) \\
& = & 1 - \left( \prob(X_{1} \le m) \cdot \ldots \cdot \prob(X_{n} \le m) \right) \\
& = & 1 - \prod_{i=1}^{n}\prob(X_{i} \le m) \\
& = & 1 - \left( \frac{1}{2} \right)^{n}.
\end{eqnarray*}
#+end_solution

* Exercise 5.7.24

#+begin_problem
Let $\randsamp$ be a random sample from a population with pdf
\begin{equation*}
f_{X}(x) =
\begin{cases}
1/\theta \quad &\text{if} \,\, 0 < x < \theta \\
0 \quad &\text{otherwise}.
\end{cases}
\end{equation*}

Let $X_{(1)} < \ldots < X_{(n)}$ be the order statistics. Show that $X_{(1)}/X_{(n)}$
and $X_{(n)}$ are independent random variables.
#+end_problem

#+begin_solution
First of all, we can see that the cdf can be immediately written as,
\begin{equation*}
F_X(x) = \frac{x}{\theta}, \quad \forall \,\, 0 < x < \theta.
\end{equation*}
The joint pdf of $X_{(i)}$ and $X_{(j)}$  is given as,

\begin{eqnarray*}
f_{X_{(1)},X_{(n)}}(x_{(1)},x_{(n)}) &=&
\frac{n!}{(n-2)!} f_{X_{(1)}}(x_{(1)}) f_{X_{(n)}}(x_{(n)})
\left[ F_{X_{(n)}}(x_{(n)}) - F_{X_{(1)}}(x_{(1)}) \right]^{n-2} \\
&=& n(n-1) \cdot \frac{1}{\theta} \cdot \frac{1}{\theta} \cdot \left[ \frac{x_{(n)}}{\theta} - \frac{x_{(1)}}{\theta} \right]^{n-2} \\
&=& \frac{n(n-1)}{\theta^n} \left( x_{(n)} - x_{(1)} \right)^{n-2}
\end{eqnarray*}

To use the Jacobian, define $u = \frac{x_{(1)}}{x_{(n)}}$ and $v = x_{(n)}$.
This gives us,

\begin{eqnarray*}
J &=&
\begin{vmatrix}
\frac{\partial{x_{(1)}}}{\partial{u}} & \frac{\partial{x_{(n)}}}{\partial{u}} \\
\frac{\partial{x_{(1)}}}{\partial{v}} & \frac{\partial{x_{(n)}}}{\partial{v}} \\
\end{vmatrix}
 \\
&=&
\begin{vmatrix}
v & 0 \\
u & 1 \\
\end{vmatrix} \\
&=& \lvert v \rvert \\
\end{eqnarray*}

Using this we now have a transformed pdf,

\begin{eqnarray*}
f_{U,V}(u,v) &=& \frac{n(n-1)}{\theta^n} (v - uv)^{n-2} \cdot \lvert J \rvert \\
&=& \frac{n(n-1)}{\theta^n} v^{n-2} (1 - u)^{n-2} \cdot v \\
&=& \frac{n(n-1)}{\theta^n} v^{n-1} (1 - u)^{n-2} \\
\end{eqnarray*}

Which we can see is separable, therefore $U$ and $V$ are independent where
$U=\frac{X_{(1)}}{X_{(n)}}$ and $V = X_{(n)}$ .
#+end_solution

* Exercise 5.7.31

#+begin_problem
Suppose $\Xbar$ is the mean of $100$ observations from a population with mean
$\mu$ and variance $\sigma^{2} = 9$. Find the limits between which $\Xbar - \mu$ will lie
with probability at least $0.9$. Use both Chebychev's Inequality and the Central
Limit Theorem, and comment on each.
#+end_problem

#+begin_solution
We can see that $\sigma_{\Xbar}^2 = \frac{9}{100}$. Using Chebychev's Inequality, we
have

\begin{equation*}
\prob(-3k/10 < \Xbar - \mu < 3k/10) \ge 1 - 1/k^2.
\end{equation*}

Since we're looking for a probability of at least $0.9$, we have $k \ge \sqrt{10}
\approx 3.16$.  Hence, we now have,

\begin{equation*}
\prob(-.949 < \Xbar - \mu < .949) \ge 0.9.
\end{equation*}

Using the Central Limit Theorem, $\Xbar$ is approximately $N(\mu, \sigma^2)$ with
$\sigma_{\Xbar} = \sqrt{0.09} = 0.3$  and,

\begin{equation*}
\frac{\Xbar - \mu}{0.3} \sim N(0,1).
\end{equation*}

Therefore,

\begin{equation*}
0.9 = \prob \left( -1.646 < \frac{\Xbar - \mu}{.3} < 1.646 \right) = \prob(-.494 < \Xbar - \mu < .494).
\end{equation*}

Comment: Chebychev's Inequality is clearly more conservative.
#+end_solution

* Exercise 5.7.33

#+begin_problem
Let $X_{n}$ be a sequence of random variables that converges in distribution to
a random variable $X$. Let $Y_{n}$ be a sequence of random variables with the
property that for any finite number $c$,

\[
\lim_{n \to \infty} \prob(Y_{n} > c) = 1.
\]

Show that for any finite number $c$,

\[
\lim_{n \to \infty} \prob(X_{n} + Y_{n} > c) = 1.
\]
#+end_problem

#+begin_solution
Choose an $m$ such that it is a continuity point of our cdf for $X_n$.  Now,
let's consider,

\begin{eqnarray*}
\prob(X_n + Y_n > c) &=& \prob((X_n + m) + (Y_n - m) > c) \\
&=& \prob((X_n + m) + (Y_n - m - c) > 0) \\
&\ge& \prob(X_n + m > 0) + \prob(Y_n - m - c > 0) - 1 \\
&\ge& \prob(X_n > -m) + \prob(Y_n > c + m) - 1 \\
\end{eqnarray*}

Now, since we have that $X_n$ converges in distribution, we can choose an
arbitrary $\ve$ such that,

\(
\prob(X_n > -m) > 1 - \frac{\ve}{2}.
\)

Similarly, since we have $lim_{n \to \infty} \prob(Y_n > c) = 1$ we can choose an
arbitrary $\ve$ such that,

\(
\prob(Y_n > c + m > 1 - \frac{\ve}{2}.
\)

Putting this together, we now have,

\begin{eqnarray*}
\prob(X_n + Y_n > c) &\ge& \prob(X_n > -m) + \prob(Y_n > c + m) - 1 \\
&>& 1 - \frac{\ve}{2} + 1 - \frac{\ve}{2} - 1 \\
&=& 1 - \ve.
\end{eqnarray*}

#+end_solution
