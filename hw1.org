#+title: Homework 1
#+date: Tuesday, January 25, 2022
#+options: toc:nil
#+latex_header: \usepackage{enumitem}
#+latex_header: \setlist[enumerate,1]{label=$\alph*)$}
#+latex_header: \usepackage{amsthm}
#+latex_header: \newenvironment{problem}{\begin{itshape}}{\end{itshape}}
#+latex_header: \newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}
#+latex_header: \newcommand{\Xbar}{\overline{X}}
#+latex_header: \allowdisplaybreaks

* Exercise 5.7.1

#+begin_problem
Color blindness occurs in $\%1$ of the people in a certain population. How large
must a sample be if the probability of its containing a color-blind person is to
be $0.95$ or more? (Assume that the population is large enough to be considered
infinite, so that sampling can be considered to be with replacement.)
#+end_problem

TODO use early lectures to show that this problem statement is $iid$

#+begin_solution
A useful insight to this problem is that "at least 1," meaning, one, two, ...,
or all people (or any subset) selected from the population ($A$) are colorblind,
has the complement set of "all zero."

Using this insight, let us formulate an easier problem statement: What is the
probability that each person is not colorblind? In other words,
\[
P\left(X_1=x_1(\text{not CB}),
X_2=x_2(\text{not CB}), \ldots, X_n=x_n(\text{not CB})\right) = ?
\]

Since this sample is =iid=,
\begin{align*}
P&\left(X_1=x_1(\text{not CB}), X_2=x_2(\text{not CB}), \ldots, X_n=x_n(\text{not CB})\right) \\
&= P(x_1) \cdot P(x_2) \cdot \ldots \cdot P(x_n) \\
&= P(\text{not CB})^n
\end{align*}

From probability theory, we have $P(A^c) = 1 - P(A)$, which we'll use to
calculate $P(\text{not CB})$. Therefore, we have $P(\text{not CB}) = 1 -
P(\text{CB}) = 1 - 0.01 = 0.99$.

From our insight, we know that,
\[
P(\text{at least one CB person}) = 1 -
P(\text{all people are not CB})
\]
Therefore,
\begin{align*}
P(\text{at least one CB person}) &= 1 - P(\text{all people are not CB}) \\
&= 1 - P(\text{person not being CB})^n \\
&= 1 - (0.99)^n
\end{align*}

We are asked how big our sample size $n$ needs to be for the $P(\text{at least
one CB person}) \ge 0.95$. This leads us to,
\begin{align*}
1 - (0.99)^n &\ge 0.95 \\
1 - 0.95 &\ge 0.99^n \\
0.99^n &\le 0.05 \\
\log 0.99^n &\le \log 0.05 \\
n \cdot \log 0.99 &\le \log 0.05 \\
\text{(log of a number} &< 1 \text{ is negative)} \\
n &\ge \frac{\log 0.05}{\log 0.99} \approx 298.072
\end{align*}

Therefore, we need a sample size of $n \ge 299$ (the smallest integer to guarantee
a $0.95$ probability).
#+end_solution

* Exercise 5.7.4
#+begin_problem
A generalization of =iid= random variables is exchangeable random variables, an
idea due to deFinetti (1972). A discussion of exchangeability can also be found
in Feller (1971). The random variables $X_{1}, \ldots, X_{n}$ are exchangeable if any
permutation of any subset of them of size $k$ ($k \leq n$) has the same
distribution.

In this exercise we will see an example of random variables that are
exchangeable but not =iid=. Let $X_{i} \mid P \sim$ =iid= $\text{Bernoulli}(P)$, $i=1,
\ldots, n$, and let $P \sim \text{uniform}(0,1)$.

1. Show that the marginal distribution of any $k$ of the $X$'s is the same as

\begin{equation}
  P\left(X_{1}=x_{1}, \ldots, X_{k}=x_{k}\right) &= \int_{0}^{1} p^{t}(1-p)^{k-t} dp \\
  &= \frac{t!(k-t) !}{(k+1) !}
\end{equation}
where $t={\displaystyle \sum_{i=1}^{k} x_{i}}$. Hence, the $X$'s are exchangeable.
#+end_problem

#+begin_solution
Recall the conditional pmf from definition 4.2.1,

\[
P(X_{1}, \ldots, X_{k} | P) = \frac{f(x_{1}, \ldots, x_{k})}{f_{X_{1},\ldots,X_{k}}(P)}
\]

where,
\[
f(x_{1}, \ldots, x_{k}) = P(X_{1}=x_{1}, \ldots, \X_{k}=x_{k})
\]

which is our left-hand side of the equation in question. First, let's tackle the
conditional probability side. From 5.1.2, we have that,
\[
P(X_{1}, \ldots, X_{k} | P) = \prod_{i=1}^{k} P(X_{i}|P)
\]
We are given that $X_{i}|P \sim \text{Bernoulli}(P)$ so we know the probability,
\[
P(X_{i}|P) = p^{x_{i}}(1 - p)^{1-x_{i}}.
\]
Putting this together, we have,
\begin{align*}
P(X_{1}, \ldots, X_{k} | P) &= \prod_{i=1}^{k} p^{x_{i}}(1 - p)^{1-x_{i}}
                       = p^{\sum_{i=1}^{k} x_{i}}(1 - p)^{\sum_{i=1}^{k}(1-x_{i})} \\
&= p^{\sum_{i=1}^{k} x_{i}}(1 - p)^{k - \sum_{i=1}^{k} x_{i}}.
\end{align*}
Using $t = {\displaystyle \sum_{i=1}^{k} x_{i}}$, we can write this as,
\[
P(X_{1}, \ldots, X_{k} | P) = p^t(1 - p)^{k - t}.
\]
Putting this together with the uniform distribution on $(0,1)$, we have
\[
f(x_{1}, \ldots, x_{k})} = P(X_{1}, \ldots, X_{k}) = \int_{0}^{1} p^t(1 - p)^{k - t} dp.
\]
This is our first result for (a). Our next step is to recognize that this is
Beta distribution. Recall the definition of the Beta distribution,
\[
B(x, y) = \int_{0}^{1}t^{x-1}(1-t)^{y-1}dt.
\]
In our case, that means our integral is a Beta distribution of $B(t+1, k-t+1)$.
From the properties of the Beta distribution we have,
\[
B(t+1, k-t+1) = \frac{\Gamma(t+1) \cdot \Gamma(k-t+1)}{\Gamma(t+1+k-t+1)} = \frac{\Gamma(t+1) \cdot \Gamma(k-t+1)}{\Gamma(k+2)}.
\]
Also recall that $\Gamma(n) = (n-1)!$ for any integer $n$, therefore,
\[
\int_{0}^{1} p^t(1 - p)^{k - t} dp = \frac{t! (k-t)!}{(k+1)!}.
\]
#+end_solution

#+begin_problem
2. [@2] Show that, marginally,
\[
  P\left(X_{1}=x_{1}, \ldots,X_{n}=x_{n}\right) \neq \prod_{i=1}^{n} P\left(X_{i}=x_{i}\right),
\]

so the distribution of the $X$'s is exchangeable but not =iid=.
#+end_problem

#+begin_solution
From our conditional probability, we have
\[
P(X_{i}=x_{i}) = P(X_{i}=x_{i}|P) \cdot P(p) = \int_{0}^{1}p^{x_{i}}(1-p)^{1-x_{i}}dp.
\]
As before, our Beta distribution gives us,
\begin{align*}
\int_{0}^{1}p^{x_{i}}(1-p)^{1-x_{i}}dp &=
\frac{\Gamma(x_{i}+1)\Gamma((1-x_{i})+1)}{\Gamma(x_{i}+1 + 1 - x_{i} + 1)} \\
&= \frac{\Gamma(x_{i}+1)\Gamma(2-x_{i})}{\Gamma(3)} \\
&= \frac{x_{i}!(1-x_{i})!}{2!}
\end{align*}

For the left-hand side, we have
\[
P(X_{1}=x_{1},\ldots,X_{n}=x_{n}) = \frac{t! (n-t)!}{(n+1)!}
\]

Now, we can see that these two quantities are not equal,
\[
\frac{t! (n-t)!}{(n+1)!} \ne \prod_{i=1}^{n} \frac{x_{i}!(1-x_{i})!}{2!}.
\]
#+end_solution

* Exercise 5.7.15

#+begin_problem
Establish the following recursion relations for means and variances. Let
$\overline{X}_n$ and $S_n^2$ be the mean and variance, respectively, of
$X_1,\ldots,X_n$. Then suppose another observation, $X_{n+1}$ becomes available. Show
that

1) $\Xbar_{n+1} = \frac{X_{n+1} + n\Xbar_n}{n + 1}$
#+end_problem

#+begin_solution
From definition $5.2.2$, we have the sample mean of sample size $n + 1$ given as

\begin{align*}
\Xbar_{n+1} &= \frac{X_1 + \ldots + X_n + X_{n+1}}{n + 1} \\
&= \frac{X_1 + \ldots + X_n}{n + 1} + \frac{X_{n+1}}{n + 1} \\
&= \frac{n}{n + 1} \cdot \frac{X_1 + \ldots + X_n}{n} + \frac{X_{n+1}}{n + 1} \\
&= \frac{n}{n + 1} \cdot \Xbar_{n} + \frac{X_{n+1}}{n + 1} \\
&= \frac{n \cdot \Xbar_{n} + X_{n+1}}{n + 1}
\end{align*}
#+end_solution

#+begin_problem
2) [@2] $nS_{n+1}^2 = (n - 1) S_n^2 + \left( \frac{n}{n+1} \right) \left(X_{n+1} -\Xbar_n \right)^2$
#+end_problem

#+begin_solution
From definition $5.2.3$, we have the sample variance of sample size $n + 1$
given as

\begin{align*}
\displaystyle
S^2 &= \frac{1}{n} \sum_{i=1}^{n+1} (X_i - \Xbar_{n+1})^2 \\
n \cdot S^2 &=\sum_{i=1}^{n+1} (X_i - \Xbar_{n+1})^2 \\
\end{align*}

From the previous calculation, we have $\Xbar_{n+1} = \frac{X_{n+1} +
n\Xbar_n}{n + 1}$.


\begin{align*}
\displaystyle
n S^2_{n+1} &= \sum_{i=1}^{n+1} \left( X_i - \frac{X_{n+1} + n\Xbar_n}{n + 1} \right)^2 \\
&= \sum_{i=1}^{n+1} \left( X_i - \Xbar_n + \Xbar_n - \frac{X_{n+1} + n\Xbar_n}{n + 1} \right)^2 \\
&= \sum_{i=1}^{n+1} \left[ \left( X_i - \Xbar_n \right) + \left( \Xbar_n - \frac{X_{n+1} + n\Xbar_n}{n + 1} \right) \right]^2 \\
&= \sum_{i=1}^{n+1} \left[ \left( X_i - \Xbar_n \right) + \left( \frac{(n+1) \Xbar_n - X_{n+1} - n\Xbar_n}{n + 1} \right) \right]^2 \\
&= \sum_{i=1}^{n+1} \left[ \left( X_i - \Xbar_n \right) + \left( \frac{\Xbar_n - X_{n+1}}{n + 1} \right) \right]^2 \\
&= \sum_{i=1}^{n+1} \left[ \left( X_i - \Xbar_n \right)^2 +
   2\left( X_i - \Xbar_n \right)\left( \frac{\Xbar_n - X_{n+1}}{n + 1} \right) +
   \left( \frac{\Xbar_n - X_{n+1}}{n + 1} \right)^2 \right] \\
&= \sum_{i=1}^{n+1} \left( X_i - \Xbar_n \right)^2 +
   \sum_{i=1}^{n+1} 2\left( X_i - \Xbar_n \right)\left( \frac{\Xbar_n - X_{n+1}}{n + 1} \right) +
   \sum_{i=1}^{n+1} \left( \frac{\Xbar_n - X_{n+1}}{n + 1} \right)^2  \\
&= \sum_{i=1}^n \left( X_i - \Xbar_n \right)^2 + \left( X_{n+1} - \Xbar_n \right)^2 \\
   & \indent + 2 \left( \frac{\Xbar_n - X_{n+1}}{n + 1} \right)
     \left[ \sum_{i=1}^n \left( X_i - \Xbar_n \right) + \left( X_{n+1} - \Xbar_n \right) \right] +
     \left( \frac{\Xbar_n - X_{n+1}}{n + 1} \right)^2 \sum_{i=1}^{n+1} 1   \\
&= (n-1)S^2_n + \left( X_{n+1} - \Xbar_n \right)^2 +
     2 \left( \frac{\Xbar_n - X_{n+1}}{n + 1} \right) \left[ 0 + \left( X_{n+1} - \Xbar_n \right) \right] \\
   & \indent + \left( \frac{\Xbar_n - X_{n+1}}{n + 1} \right)^2 (n + 1)\\
&= (n-1)S^2_n + \left( X_{n+1} - \Xbar_n \right)^2 -
     2 \frac{\left( \Xbar_n - X_{n+1} \right)^2}{n + 1} +
     \frac{\left( \Xbar_n - X_{n+1} \right)^2}{n + 1} \\
&= (n-1)S^2_n + ( X_{n+1} - \Xbar_n )^2 - \frac{(X_{n+1} - \Xbar_n)^2}{n + 1} \\
&= (n-1)S^2_n + ( X_{n+1} - \Xbar_n )^2 \left( 1 - \frac{1}{n + 1} \right) \\
&= (n-1)S^2_n + ( X_{n+1} - \Xbar_n )^2 \left( \frac{n + 1 - 1}{n + 1} \right) \\
&= (n-1)S^2_n + ( X_{n+1} - \Xbar_n )^2 \left( \frac{n}{n + 1} \right)
\end{align*}
#+end_solution

* (Optional) 5.7.5

#+begin_problem
Let $X_1, \ldots, X_n$ be =iid= with pdf $f_X(x)$, and let $\Xbar$ denote the sample
mean. Show that $f_{\Xbar} = nf_{X_1 + \ldots + X_n} (nx)$, even if the =mgf= of $X$
does not exist.
#+end_problem

#+begin_solution
Let $Y=X_{1} + \ldots + X_{n}$. Then by transformation, $\Xbar = \frac{1}{n}Y$.
Hence, the pdf of $\Xbar$ is
\[
f_{\Xbar} = \frac{1}{\frac{1}{n}}f_{Y}\left(\frac{x}{\frac{1}{n}}\right) = n
\cdot f_{Y}(n \cdot x).
\]
#+end_solution
